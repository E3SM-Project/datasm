The utilities or tools contained herein are not strictly necessary for datasm operation,
and are not called upon by the datasm system.  They are nonetheless useful for managing
E3SM data operations.  Their functions are briefly described below:

NOTE:  These tools are "installed" to /p/user_pub/e3sm/staging/tools, and many rely upon
definition files (Archive_Map, dataset_spec.yam) in /p/user_pub/e3sm/staging/resource.

NOTE:  Much of E3SM data operations involve "datasets", and in that regard, dataset_ids
are ubiquitous and employed as "tokens" for many operations.  Single dataset_ids, or
files containing lists of dataset_ids, are often among the parameters given to a tool to
accomplish a given task.  Hence the utilities "list_e3sm_dsids" and "list_cmip_dsids",
each of which employ the dataset-defining "dataset_spec.yaml" are often filtered down to
an appropriate list of dataset_ids as a first step in condicting operations.

IMPORTANT CONFIGURATION FILES:

    /p/user_pub/e3sm/archive/.cfg/Archive_Locator
    /p/user_pub/e3sm/archive/.cfg/Archive_Map
    /p/user_pub/e3sm/archive/.cfg/Standard_Datatype_Extraction_Patterns
    /p/user_pub/e3sm/staging/resource/dataset_spec.yaml
    /p/user_pub/e3sm/staging/resource/table_cmip_var_to_e3sm

ALPHABETIC LISTING OF TOOLS:

archive_dataset_extractor.sh:

    archive_dataset_extractor.sh infile [dest_directory]

    Accepts a file of ONE line from the Archive_Map, and will extract the corresponding
    datasets to the dest_directory (or just list them, if no directory is given.)

archive_dataset_extractor.py:

    python archive_dataset_extractor -a am_specfile [-d dest_dir] [-O]

    Similar to the bash script "archive_dataset_extractor.sh".  Use "--help" for details

archive_extraction_service.py:

    nohup python archive_extraction_service.py &

    Runs much as a background daemon to service "extraction request tickets" placed in

        /p/user_pub/e3sm/archive/.extraction_requests_pending/
        /p/user_pub/e3sm/archive/.extraction_requests_processed/  (moved here when done)

    These tickets are automatically generated by "datasm_extract_from_archive.sh",
    which is designed to restart the archive_extraction_service if it is not running.
    Corresponding datasets are extracted to the warehouse (/p/user_pub/e3sm/warehouse).

atchive_path_mapper.py

    archive_path_mapper -a al_listfile [-s sdepfile]

    Accepts a selected list of lines (from /p/user_pub/e3sm/archive/.cfg/Archive_Locator)
    and for each line, creates a cross-product with each datatype matching pattern found
    in the (archive/.cfg/) Standard_Datatype_Extraction_Patterns (or a supplied subset).
    Each pattern is applied to the indicated zstash archive to produce all fund lists of
    pattern-matched files.  A body of preliminary "Archive_Map" lines are generated, each
    of which must be reviewed and manually completed in order to upda the Archive_Map.

arch_loc_from_arch_map.sh

    arch_loc_from_arch_map.sh am_lines

    Accepts a selection of Archive_Map lines, and gives the set of Archive_Locator lines
    that correspond The dataset types (realm, grid, frequency) are lost.  Good for the
    setup of "zstash holodeck" symlinks in the manual exploration of archives.

consolidated_cmip_dataset_report.py
consolidated_e3sm_dataset_report.py

    These generally take no arguments, and survey the Archive_Map, dataset_spec.yaml,
    the warehouse filesystem, the publication filesystem, and the esgf search nodes in
    order to determine and report the status of each dataset that exists under the E3SM.
    The output is CSV, and (with a little work) can produce an Excel Spreadsheet that
    can be sorted on each of 20+ fields.

contract_dataset_spec.py

    contract_dataset_spec -i expanded_dataset_spec -o contracted_dataset_spec

    Accepts a standard form of the dataset_spec (aka "expanded") and will take each
    dataset specification "tree", isolate the "case_extensions" branch, and create
    a consolidated unique "CASE_EXTENSIONS" tree, reducing each dataset_tree to its
    global parameters plus a "Case_Extension_ID" in place of the original extensions.
    This makes it much easier to edit the branches (reduced form about 60 to 12) and
    easier to compare and contrast the major dataset definitions, and news ones, etc.
    See "expand_dataset_spec.py" for the opposite transform.

datasm_extract_from_archive.sh


datasm_pp_sourceroot.sh


datasm_verify_publication.py


dsids_to_archive_map_keys.sh


dsids_to_archive_map_lines.sh


ds_paths_info_dsid_list_compact.sh


ds_paths_info_dsid_list.sh


ds_paths_info.sh


ensure_status_file_for_dsid.sh


expand_dataset_spec.py


get_e3sm_vars_for_cmip.sh


latest_data_location_by_dsid.sh

    For a given input dataset_id, will scour both the warehouse and publication
    filesystems to the latest populated version directory.  The full path to the
    best location is returned, or else "NONE" if not populated directory is found.


list_cmip6_dsids.py
list_e3sm_dsids.py

    These take no parameters, and will output ALL CMIP6 or E3SM dataset_ids, which
    may then be filtered with "grep" or "cut" for many applications.


metadata_version.py


parent_native_dsid.sh


report_first_file_for_latest_on_dsid_list.sh


restart_services.sh


rw_yaml.py


tell_status_last.py


tell_years_dsid.py


trisect.py






    (work in progress) 
    



