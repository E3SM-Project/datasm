import os, sys
import argparse
import re
from argparse import RawTextHelpFormatter
import yaml
import time
from datetime import datetime

'''
The Big Idea:  Create a dictionary "ds_struct[]" keyed by dataset_ID, whose values will be the desired
output fields of the report:

    Campaign, Model, Experiment, Resolution, Ensemble, DatasetType, Realm, Grid, Freq, DAWPS, D, A, W, P, S,
        StatDate, Status, W_Version, W_Count, P_Version, P_Count, S_Version, S_Count, W_Path, P_Path

For each of these 5 sources of dataset_IDs:

    {dataset_spec, archive_map, warehouse_dirs, publication_dirs, sproket_esgf_search}

construct a list of all obtainable dataset_IDs. For each dataset_ID in the list, seek that entry in the
ds_struct. Update the entry (adding new if not found) with data appropriate to the section being processed.
'''

helptext = '''
    Usage:  awps_dataset_status -s sproket_publication_report

        The awps_dataset_status is produced by plying 4 sources to determine whether datasets exist
        in any of (Archive,Warehouse,PubDirs,SproketPubReport), hereafter (A,W,P,S).

        Existence is determined by:

        (A):  Appearance in the Archive_Map (/p/user_pub/e3sm/archive/.cfg/Archive_Map)
        (W):  The Warehouse directories (/p/user_pub/e3sm/warehouse/E3SM/(facets)/[v0|v1...]/
        (W):  The Publication directories (/p/user_pub/work/E3SM/(facets)/[v0|v1...]/
        (S):  The provided sproket_publication_report

        The prerequisite sproket_publication_report is generated by running the scripts

            sproket_search.sh   (produces E3SM_datafile_urls-<date>)
            process_sproket_output.sh (takes above output and produces ESGF_publication_report-<date>)

'''

# INPUT FILES (These could be in a nice config somewhere.  staging/.paths
staging_paths = '/p/user_pub/e3sm/staging/.paths'

PB_ROOT = '/p/user_pub/work'
WH_ROOT = '/p/user_pub/e3sm/warehouse'
DS_SPEC = '/p/user_pub/e3sm/staging/resource/dataset_spec.yaml'
DS_STAT = '/p/user_pub/e3sm/staging/status'

ARCH_MAP  = '/p/user_pub/e3sm/archive/.cfg/Archive_Map'
esgf_pr   = ''

# output_mode
gv_csv = True

# esgf_pr   = '/p/user_pub/e3sm/bartoletti1/Pub_Status/sproket/ESGF_publication_report-20200915.144250'

def ts():
    return datetime.now().strftime('%Y%m%d_%H%M%S')

def assess_args():
    global thePWD
    global esgf_pr
    global gv_csv

    thePWD = os.getcwd()
    # parser = argparse.ArgumentParser(description=helptext, prefix_chars='-')
    parser = argparse.ArgumentParser(description=helptext, prefix_chars='-', formatter_class=RawTextHelpFormatter)
    parser._action_groups.pop()
    required = parser.add_argument_group('required arguments')
    optional = parser.add_argument_group('optional arguments')

    required.add_argument('-s', action='store', dest="esgf_pr", type=str, required=True)

    args = parser.parse_args()

    esgf_pr = args.esgf_pr

# Generic Convenience Functions =============================

def loadFileLines(afile):
    retlist = []
    if len(afile):
        with open(afile,"r") as f:
            retlist = f.read().split('\n')
        retlist = [ _ for _ in retlist if _[:-1] ]
    return retlist



#### BEGIN rationalizing archive and publication experiment-case names, and dataset-type names ####
# dsid = proj.model.experiment.resolution[.tuning].realm.grid.outtype.freq.ens.ver
# SPECIAL CASES:  grid          out_type        freq    ->  realm_grid_freq
# climo:          

def get_dsid_dstype( dsid ):  # only works because "tuning" comes before realm, grid and frequency
    comps=dsid.split('.')
    realm = comps[-5]
    gridv = comps[-4]   # may be overloaded with "namefile" or "restart"
    otype = comps[-3]
    freq = comps[-2]    # will be "fixed" for gridv in "namefile" or "restart"

    if realm == 'atmos':
        realm = 'atm'
    elif realm == 'land':
        realm = 'lnd'
    elif realm == 'ocean':
        realm = 'ocn'

    grid = gridv
    if gridv == 'native':
        grid = 'nat'
    elif otype == 'monClim':    # should be "climo"
        freq = 'mon'
    elif otype == 'seasonClim': # should be "climo"
        freq = 'season'
    elif gridv == 'restart' or gridv == 'namefile' or gridv == 'namelist':
        freq = 'fixed'
    else:
        grid = gridv
    return '_'.join([realm,grid,freq])

#### COMPLETED rationalizing archive and publication experiment-case names, and dataset-type names ####

def isVLeaf(_):
    if len(_) > 1 and _[0] == 'v' and _[1] in '0123456789':
        return True
    return False

def get_maxv_info(edir):
    ''' given an ensemble directory, return the max "v#" subdirectory and its file count '''
    tuplist = []
    for root, dirs, files in os.walk(edir):
        if not dirs:
            tuplist.append(tuple([os.path.split(root)[1],len(files)]))
    tuplist.sort()
    return tuplist[-1]
    
def get_leaf_dirs_by_walk(rootpath,project):
    leaf_dirs = []
    seekpath = os.path.join(rootpath,project)
    for root, dirs, files in os.walk(seekpath):
        if not dirs:     # at leaf-directory
            leaf_dirs.append(root)
    return leaf_dirs

def get_dataset_path_tuples(rootpath):
    leaf_dirs = get_leaf_dirs_by_walk(rootpath,'E3SM')
    path_tuples = list()
    for adir in leaf_dirs:
        ensdir, vleaf = os.path.split(adir)
        if not isVLeaf(vleaf):
            continue
        for root, dirs, files in os.walk(adir):
            if files:
                path_tuples.append( tuple([ensdir,vleaf,len(files)]))
            else:
                path_tuples.append( tuple([ensdir,vleaf,0]))
    return path_tuples
                
def bookend_files(adir):
    for root, dirs, files in os.walk(adir):
        if files:
            return sorted(files)[0], sorted(files)[-1]
    return "", ""

def get_statfile_path(dsid):
    s_path = os.path.join(DS_STAT,dsid + '.status')
    if os.path.exists(s_path):
        return s_path
    return ""

# sf_status = get_sf_laststat(epath)

def get_sf_laststat(dsid):
    sf_path = get_statfile_path(dsid)
    if sf_path == '':
        return ':NO_STATUS_FILE_PATH'
    sf_list = loadFileLines(sf_path)
    if len(sf_list) == 0:
        return ':EMPTY_STATUS_FILE'
    sf_last = sf_list[-1]
    last_stat = ':'.join(sf_last.split(':')[1:])
    return last_stat

''' NEW STUFF ====================================================================================================

ds_struct[] will be keyed by FULL DSID.  The Values with be a dictionary of 

    Campaign, Model, Experiment, Resolution, Ensemble, Output_Type, DatasetType, Realm, Grid, Freq, AWPS, A, W, P, S, StatDate, Status, W_Version, W_Count, P_Version, P_Count, S_Version, S_Count, W_Path, P_Path, FirstFile, LastFile

'''
 
# ==== new stuff ==== #

def new_ds_record():
    return { 'project':'', 'campaign':'', 'model':'', 'experiment':'', 'ensemble':'', 'output_type':'', 'datasettype':'', 'realm':'', 'grid':'', 'frequency':'', 'DAWPS':'', 'D':'_', 'A':'_', 'W':'_', 'P':'_', 'S':'_', 'StatDate':'', 'Status':'', \
        'W_Version':'', 'W_Count':0, 'P_Version':'', 'P_Count':0, 'S_Version':'', 'S_Count':0, 'W_Path':'', 'P_Path':'', 'FirstFile':'', 'LastFile':'' }

def realm_grid_freq_from_dstype(dstype):
    dstlist = dstype.split('_')
    rcode = dstlist[0]
    gcode = dstlist[1]
    if len(dstlist) == 3:
        freq = dstlist[2]
    else:
        freq = '_'.join([dstlist[2], dstlist[3]])

    if rcode == 'atm':
        realm = 'atmos'
    elif rcode == 'lnd':
        realm = 'land'
    elif rcode == 'ocn':
        realm = 'ocean'
    else:
        realm = rcode

    if gcode == 'nat':
        grid = 'native'
    else:
        grid = gcode

    return realm, grid, freq

def campaign_via_model_experiment(model,experiment):
    if model in ['1_0']:
        if experiment in ['1950-Control-HR','1950-Control-LR','1950-Control-LRtunedHR','1950-Control-21yrContHiVol-HR',\
            'F2010-HR','F2010-LR','F2010-LRtunedHR','F2010-nudgeUV-HR','F2010-nudgeUV-LR','F2010-nudgeUV-LRtunedHR',\
            'F2010-nudgeUV-1850aero-HR','F2010-nudgeUV-1850aero-LR','F2010-nudgeUV-1850aero-LRtunedHR',\
            'F2010-plus4k-HR','F2010-plus4k-LR','F2010-plus4k-LRtunedHR']:
            return 'HR-v1'
        return 'DECK-v1'
    elif model in ['1_1','1_1_ECA']:
        return 'BGC-v1'
    elif model in ['1_2','1_2_1','1_3']:
        return 'CRYO'
    else:
        return "UNKNOWN_CAMPAIGN"

# ==== generate dsids from dataset spec

def collect_e3sm_datasets(dataset_spec):
    for version in dataset_spec['project']['E3SM']:
        for experiment, experimentinfo in dataset_spec['project']['E3SM'][version].items():
            for ensemble in experimentinfo['ens']:
                for res in experimentinfo['resolution']:
                    for comp in experimentinfo['resolution'][res]:
                        for item in experimentinfo['resolution'][res][comp]:
                            for data_type in item['data_types']:
                                if item.get('except') and data_type in item['except']:
                                    continue
                                dataset_id = f"E3SM.{version}.{experiment}.{res}.{comp}.{item['grid']}.{data_type}.{ensemble}"
                                yield dataset_id


def dsids_from_dataset_spec(dataset_spec_path):
    with open(dataset_spec_path, 'r') as instream:
        dataset_spec = yaml.load(instream, Loader=yaml.SafeLoader)
        # cmip6_ids = [x for x in collect_cmip_datasets(dataset_spec)]
        e3sm_ids = [x for x in collect_e3sm_datasets(dataset_spec)]
        # dataset_ids = cmip6_ids + e3sm_ids

    return e3sm_ids
    # return dataset_ids



# DSID = PROJ.Model.Exper.Resol.[tuning.]Realm.Grid.OutType.Freq.Ens

def dsid_from_archive_map(amline):
    #   Campaign,Model,Experiment,Resolution,Ensemble,DatasetType,ArchivePath,DatatypeTarExtractionPattern,Notes
    amap_items = amline.split(',')
    realm, grid, freq = realm_grid_freq_from_dstype(amap_items[5])
    tuning = 0
    if amap_items[2][0:5] == 'F2010':
        tuning = 1
    if amap_items[2][0:12] == '1950-Control' and not amap_items[2] == '1950-Control-21yrContHiVol-HR':
        amap_items[2] = '1950-Control'
    dsid_items = list()
    dsid_items.append('E3SM')
    dsid_items.append(amap_items[1]) # model
    dsid_items.append(amap_items[2]) # exper
    dsid_items.append(amap_items[3]) # resol
    if tuning:
        dsid_items.append('tuning')     # should not happen from archive_map
    dsid_items.append(realm)
    dsid_items.append(grid)
    dsid_items.append('model-output')
    dsid_items.append(freq)
    dsid_items.append(amap_items[4]) # ensem
        
    dsid = '.'.join(dsid_items)
    return dsid

def dict_from_dsid(dsid):
    dsiddict = dict()
    dsidlist = dsid.split('.')
    dsiddict['project'] = dsidlist[0]
    dsiddict['model'] = dsidlist[1]
    dsiddict['experiment'] = dsidlist[2]
    dsiddict['resolution'] = dsidlist[3]
    if len(dsidlist) > 9:
        dsiddict['tuning'] = dsidlist[4]
        dsiddict['realm'] = dsidlist[5]
        dsiddict['grid'] = dsidlist[6]
        dsiddict['outputtype'] = dsidlist[7]
        dsiddict['frequency'] = dsidlist[8]
        dsiddict['ensemble'] = dsidlist[9]
    else:
        dsiddict['tuning'] = ''
        dsiddict['realm'] = dsidlist[4]
        dsiddict['grid'] = dsidlist[5]
        dsiddict['outputtype'] = dsidlist[6]
        dsiddict['frequency'] = dsidlist[7]
        dsiddict['ensemble'] = dsidlist[8]

    return dsiddict

        
def dsids_from_archive_map(arch_map):
    # split the Archive_Map into a list of records, each record a list of fields
    #   Campaign,Model,Experiment,Resolution,Ensemble,DatasetType,ArchivePath,DatatypeTarExtractionPattern,Notes
    contents = loadFileLines(arch_map)
    am_list = [ aline.split(',') for aline in contents if aline[:-1] ]
    dsid_list = list()
    for am_line in am_list:
        dsid_list.append(dsid_from_archive_map(am_line))

    return dsid_list


def dsid_from_warehouse_path(whpath):
    return '.'.join(whpath.split(os.sep)[5:])

def dsid_from_publication_path(pbpath):
    return '.'.join(pbpath.split(os.sep)[4:])

def dsid_from_sproket_rep(vline):
    return '.'.join(vline.split('.')[:-1])

def init_ds_record_from_dsid(dsrec,dsid):
    dsiddict = dict_from_dsid(dsid)
    for key in dsiddict.keys():
        dsrec[key] = dsiddict[key]
    dsrec['datasettype'] = get_dsid_dstype(dsid)

def dumplist(alist):
    for item in alist:
        print(f'DUMPING: {item}', flush = True)

def report_ds_struct(ds_struct):
    out_line = 'Campaign,Model,Experiment,Resolution,Ensemble,OutputType,DatasetType,Realm,Grid,Freq,DAWPS,D,A,W,P,S,StatDate,Status,W_Version,W_Count,P_Version,P_Count,S_Version,S_Count,W_Path,P_Path,FirstFile,LastFile'
    print(f'{out_line}')
        
    for dsid in ds_struct:
        ds = ds_struct[dsid]
        out_list = []
        out_list.append(ds['campaign'])
        out_list.append(ds['model'])
        out_list.append(ds['experiment'])
        out_list.append(ds['resolution'])
        out_list.append(ds['ensemble'])
        out_list.append(ds['outputtype'])
        out_list.append(ds['datasettype'])
        out_list.append(ds['realm'])
        out_list.append(ds['grid'])
        out_list.append(ds['frequency'])
        out_list.append(ds['DAWPS'])
        out_list.append(ds['D'])
        out_list.append(ds['A'])
        out_list.append(ds['W'])
        out_list.append(ds['P'])
        out_list.append(ds['S'])
        out_list.append(ds['StatDate'])
        out_list.append(ds['Status'])
        out_list.append(ds['W_Version'])
        out_list.append(str(ds['W_Count']))
        out_list.append(ds['P_Version'])
        out_list.append(str(ds['P_Count']))
        out_list.append(ds['S_Version'])
        out_list.append(str(ds['S_Count']))
        out_list.append(ds['W_Path'])
        out_list.append(ds['P_Path'])
        out_list.append(ds['FirstFile'])
        out_list.append(ds['LastFile'])
        out_line = ','.join(out_list)
        print(f'{out_line}', flush=True)


debug = False

''' build
    Project, Campaign, Model, Experiment, Resolution, Ensemble, OutputType, DatasetType, Realm, Grid, Freq, DAWPS, D, A, W, P, S,
        StatDate, Status, W_Version, W_Count, P_Version, P_Count, S_Version, S_Count, W_Path, P_Path, FirstFile, LastFile
'''

def main():

    assess_args()

    ds_struct = dict()

    ''' STAGE 0:  Create initial set of ds_struct[] entries from the dataset_spec. '''

    dsids_stage_1 = dsids_from_dataset_spec(DS_SPEC)
    for dsid in dsids_stage_1:
        ds_struct[dsid] = new_ds_record()
        ds = ds_struct[dsid]
        # dsid = proj.model.experiment.resolution[.tuning].realm.grid.outtype.freq.ens.ver
        init_ds_record_from_dsid(ds, dsid)
        ds['D'] = 'D'

    ''' STAGE 1:  Update ds_struct with datasets found in the archive_map. '''

    # split the Archive_Map into a list of records, each record a list of fields
    #   Campaign,Model,Experiment,Resolution,Ensemble,DatasetType,ArchivePath,DatatypeTarExtractionPattern,Notes
    am_lines = loadFileLines(ARCH_MAP)
    for amline in am_lines:
        dsid = dsid_from_archive_map(amline)
        if not dsid in ds_struct:
            ds_struct[dsid] = new_ds_record()
            init_ds_record_from_dsid(ds_struct[dsid],dsid)
        ds = ds_struct[dsid]
        ds['A'] = 'A'

    ''' STAGE 2:  Walk the warehouse.  Collect dataset path, max version, and filecount of max version '''

    wh_path_tuples = get_dataset_path_tuples(WH_ROOT)   # list of (ensembledir,vleaf,filecount)
    dataset_paths = list(set([ atup[0] for atup in wh_path_tuples ]))
    for w_path in dataset_paths:
        dsid = dsid_from_warehouse_path(w_path)
        if not dsid in ds_struct:
            ds_struct[dsid] = new_ds_record()
            init_ds_record_from_dsid(ds_struct[dsid],dsid)
        ds = ds_struct[dsid]

        ds['W'] = 'W'
        maxv, maxc = get_maxv_info(w_path)
        ds['W_Path'] = w_path
        ds['W_Version'] = maxv
        ds['W_Count'] = maxc

    ''' STAGE 3:  Walk the publication dirs. Collect dataset path, max version, and filecount of max version '''

    pb_path_tuples = get_dataset_path_tuples(PB_ROOT)   # list of (ensembledir,vleaf,filecount)
    # dumplist(pb_path_tuples); # DEBUG
    dataset_paths = list(set([ atup[0] for atup in pb_path_tuples ]))
    for p_path in dataset_paths:
        dsid = dsid_from_publication_path(p_path)
        if not dsid in ds_struct:
            ds_struct[dsid] = new_ds_record()
            init_ds_record_from_dsid(ds_struct[dsid],dsid)
        ds = ds_struct[dsid]

        ds['P'] = 'P'
        maxv, maxc = get_maxv_info(p_path)
        ds['P_Path'] = p_path
        ds['P_Version'] = maxv
        ds['P_Count'] = maxc

    ''' STAGE 4: Process the supplied (latest) sproket-generated ESGF_publication_report.  Collect max version, and filecount of max version. '''

    contents = loadFileLines(esgf_pr)
    esgf_pr_list = [ aline.split(',') for aline in contents if aline[:-1] and not aline[0] == 'NOMATCH' ] # files, years, dsid+vers, first_file
    for aline in esgf_pr_list:
        filecount = aline[1]
        vline = aline[2]
        vers = vline.split('.')[-1]
        dsid = dsid_from_sproket_rep(vline)
        if not dsid in ds_struct:
            ds_struct[dsid] = new_ds_record()
            init_ds_record_from_dsid(ds_struct[dsid],dsid)
        ds = ds_struct[dsid]
        
        ds['S'] = 'S'
        ds['S_Version'] = vers
        ds['S_Count'] = filecount

    ''' STAGE 5: Set Campaign, Seek a status file for each ds_struct entry, enter as "date" and "status", set AWPS code '''

    for dsid in ds_struct:
        ds = ds_struct[dsid]
        
        ds['campaign'] = campaign_via_model_experiment(ds['model'],ds['experiment'])
        sf_data = get_sf_laststat(dsid)
        ds['StatDate'] = sf_data.split(':')[0]                # date from last status value
        ds['Status']  = ':'.join(sf_data.split(':')[1:])      # stat from last status value
        ds['DAWPS'] = ds['D']+ds['A']+ds['W']+ds['P']+ds['S']

    # first file and last file of highest version in pub, else in warehouse

    for pb_tup in pb_path_tuples:
        if pb_tup[2] == 0:
            continue
        pb_path = os.path.join(pb_tup[0],pb_tup[1])
        ffile, lfile = bookend_files(pb_path)
        if len(ffile) and len(lfile):
            dsid = dsid_from_publication_path(pb_tup[0])
            ds = ds_struct[dsid]
            ds['FirstFile'] = ffile
            ds['LastFile'] = lfile

    for wh_tup in wh_path_tuples:
        if wh_tup[2] == 0:
            continue
        dsid = dsid_from_warehouse_path(wh_tup[0])
        ds = ds_struct[dsid]
        if len(ds['FirstFile']):
            continue
        wh_path = os.path.join(wh_tup[0],wh_tup[1])
        ffile, lfile = bookend_files(wh_path)
        if len(ffile) and len(lfile):
            ds['FirstFile'] = ffile
            ds['LastFile'] = lfile
        
    ''' Print the Report '''

    report_ds_struct(ds_struct)
            
    sys.exit(0)

if __name__ == "__main__":
  sys.exit(main())

