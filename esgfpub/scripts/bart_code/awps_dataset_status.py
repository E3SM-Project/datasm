import os, sys
import argparse
import re
from argparse import RawTextHelpFormatter
import time
from datetime import datetime

'''
The Big Idea:  Using the Archive_Map and the (sproket-based) ESGF publication report,
produce from each the list of "canonical" experiment/dataset "keys" where

	akey = ( <model>, <experiment>, <resolution>, <ensemble> )
and
	dkey = <realm_grid_freq>

Use these to index a dictionary structure "dataset_status[akey][dkey] = ( A: bool, W: bool, P: bool, S: bool )
with the Boolean values set according to the presence or absence of the key-pair in either of the
two input files (archive map, esgf publication report), or the warehouse or publication directories.
'''

helptext = '''
    Usage:  awps_dataset_status -s sproket_publication_report

        The awps_dataset_status is produced by plying 4 sources to determine whether datasets exist
        in any of (Archive,Warehouse,PubDirs,SproketPubReport), hereafter (A,W,P,S).

        Existence is determined by:

        (A):  Appearance in the Archive_Map (/p/user_pub/e3sm/archive/.cfg/Archive_Map)
        (W):  The Warehouse directories (/p/user_pub/e3sm/warehouse/E3SM/(facets)/[v0|v1...]/
        (W):  The Publication directories (/p/user_pub/work/E3SM/(facets)/[v0|v1...]/
        (S):  The provided sproket_publication_report

        The prerequisite sproket_publication_report is generated by running the scripts

            sproket_search.sh   (produces E3SM_datafile_urls-<date>)
            process_sproket_output.sh (takes above output and produces ESGF_publication_report-<date>)

'''

# INPUT FILES
arch_map  = '/p/user_pub/e3sm/archive/.cfg/Archive_Map'
warehouse = '/p/user_pub/e3sm/warehouse/E3SM'
publicati = '/p/user_pub/work/E3SM'
esgf_pr   = ''

# output_mode
gv_csv = False

# esgf_pr   = '/p/user_pub/e3sm/bartoletti1/Pub_Status/sproket/ESGF_publication_report-20200915.144250'

def assess_args():
    global thePWD
    global esgf_pr
    global gv_csv

    thePWD = os.getcwd()
    # parser = argparse.ArgumentParser(description=helptext, prefix_chars='-')
    parser = argparse.ArgumentParser(description=helptext, prefix_chars='-', formatter_class=RawTextHelpFormatter)
    parser._action_groups.pop()
    required = parser.add_argument_group('required arguments')
    optional = parser.add_argument_group('optional arguments')

    required.add_argument('-s', action='store', dest="esgf_pr", type=str, required=True)
    optional.add_argument('--csv', action='store_true', dest="csv", required=False)

    args = parser.parse_args()

    esgf_pr = args.esgf_pr
    if args.csv:
        gv_csv = args.csv

# Generic Convenience Functions =============================

def loadFileLines(afile):
    retlist = []
    if len(afile):
        with open(afile,"r") as f:
            retlist = f.read().split('\n')
        retlist = [ _ for _ in retlist if _[:-1] ]
    return retlist



#### BEGIN rationalizing archive and publication experiment-case names, and dataset-type names ####
# dsid = proj.model.experiment.resolution[.tuning].realm.grid.outtype.freq.ens.ver

# call to specialize a publication experiment name to an archive experiment name

def specialize_expname(expn, reso, tune):
    if expn == 'F2010plus4k':
        expn = 'F2010-plus4k'
    if expn[0:5] == 'F2010' or expn == '1950-Control':
        if reso[0:4] == '1deg' and tune == 'highres':
            expn = expn + '-LRtunedHR'
        else:
            expn = expn + '-HR'
    return expn

def get_idval(ensdir):
    return '.'.join(ensdir.split(os.sep)[5:])
    # should be from /p/user_pub/e3sm/warehouse/E3SM/model/...

def get_dsid_arch_key(dsid):    # dsid:  see above 
    # print(f'DSID: {dsid}')
    comps=dsid.split('.')       # E3SM model exper resol (realm|tuning) . . . ensemble
    expname = specialize_expname(comps[2],comps[3],comps[4])
    return comps[1],expname,comps[3],comps[-1]

def get_dsid_type_key( dsid ):
    comps=dsid.split('.')
    realm = comps[-5]
    gridv = comps[-4]   # may be overloaded with "namefile" or "restart"
    otype = comps[-3]
    freq = comps[-2]    # will be "fixed" for gridv in "namefile" or "restart"

    if realm == 'atmos':
        realm = 'atm'
    elif realm == 'land':
        realm = 'lnd'
    elif realm == 'ocean':
        realm = 'ocn'

    if gridv == 'native':
        grid = 'nat'
    elif otype == 'climo':
        grid = 'climo'
    elif otype == 'monClim':
        grid = 'climo'
        freq = 'mon'
    elif otype == 'seasonClim':
        grid = 'climo'
        freq = 'season'
    elif otype == 'time-series':
        grid = gridv
        freq = 'ts-' + freq
    elif gridv == 'namefile':
        grid = 'namefile'
        freq = 'fixed'
    elif gridv == 'restart':
        grid = 'restart'
        freq = 'fixed'
    else:
        grid = gridv
    return '_'.join([realm,grid,freq])

#### COMPLETED rationalizing archive and publication experiment-case names, and dataset-type names ####

def isVLeaf(_):
    if len(_) > 1 and _[0] == 'v' and _[1] in '0123456789':
        return True
    return False

def get_maxv_info(edir):
    ''' given an ensemble directory, return the max "v#" subdirectory and its file count '''
    tuplist = []
    for root, dirs, files in os.walk(edir):
        if not dirs:
            tuplist.append(tuple([os.path.split(root)[1],len(files)]))
    tuplist.sort()
    return tuplist[-1]
    
def dataset_print_csv( akey, dkey, newln ):
    outstr = f'{akey[0]},{akey[1]},{akey[2]},{dkey}'
    if gv_csv:
        if newln == 1:
            print(f'{outstr},')
        else:
            print(f'{outstr},', end = '')
    else:
        if newln == 1:
            print(f'{outstr:60}')
        else:
            print(f'{outstr:60}', end = '')


def get_dataset_dirs_loc(anydir,loc):   # loc in ['P','W']

    '''
        Return tuple (ensemblepath,[version_paths])
        for the dataset indicated by "anydir" whose
        "dataset_id" part identifies a dataset, and
        whose root part is warehouse or publication.
    '''

    ''' WARNING: HARDCODED PATHS '''
    WH_root = "/p/user_pub/e3sm/warehouse"
    PB_root = "/p/user_pub/work"

    if not loc in ['P','W']:
        print(f'ERROR: invalid dataset location indicator:{loc}')
        return '',[]
    if not (WH_root in anydir or PB_root in anydir):
        print(f'ERROR: invalid dataset source path - bad root:{anydir}')
        return '',[]
    if WH_root in anydir:
        ds_part = anydir[1+len(WH_root):]
    else:
        ds_part = anydir[1+len(PB_root):]

    tpath, leaf = os.path.split(ds_part)

    if len(leaf) == 0:
        tpath, leaf = os.path.split(tpath)
    if leaf[0] == 'v' and leaf[1] in '0123456789':
        ens_part = tpath
    elif (leaf[0:3] == 'ens' and leaf[3] in '123456789'):
        ens_part = ds_part
    else:
        print(f'ERROR: invalid dataset source path:{anydir}')
        return '',[]

    if loc == 'P':
        a_enspath = os.path.join(PB_root, ens_part)
    else:
        a_enspath = os.path.join(WH_root, ens_part)

    vpaths = []
    if os.path.exists(a_enspath):
        vpaths = [ f.path for f in os.scandir(a_enspath) if f.is_dir() ]      # use f.path for the fullpath
        vpaths.sort()

    # print(f'DEBUG: get_dataset_dirs_loc: RETURNING: a_enspath = {a_enspath}, vpaths = {vpaths}',flush=True)
    return a_enspath, vpaths

def get_statfile_path(ds_dir):
    w_enspath, _ = get_dataset_dirs_loc(ds_dir,'W')
    p_enspath, _ = get_dataset_dirs_loc(ds_dir,'P')

    if w_enspath == '' and p_enspath == '':
        return ''

    w_sf = os.path.join(w_enspath,'.status')
    p_sf = os.path.join(p_enspath,'.status')

    if not os.path.exists(w_sf) and not os.path.exists(p_sf):
        print(f'ERROR: no status file can be found for ds_dir {ds_dir}', file=sys.stderr)
        return 'ERROR:NO_STATUS_FILE_PATH'
    if os.path.exists(w_sf) and os.path.exists(p_sf):
        print(f'ERROR: ambiguous status files exist: {ds_dir}', file=sys.stderr)
        return 'ERROR:AMBIGUOUS_STATUS_FILES'
    
    if os.path.exists(w_sf):
        return w_sf

    return p_sf


# sf_status = get_sf_laststat(epath)

def get_sf_laststat(epath):
    if epath == '':
        return ':NO_STATUS_FILE_PATH'
    sf_path = get_statfile_path(epath)
    if sf_path == '':
        return ':NO_STATUS_FILE_PATH'
    if sf_path[0:5] == 'ERROR':
        return sf_path[5:]
    sf_list = loadFileLines(sf_path)
    sf_last = sf_list[-1]
    last_stat = ':'.join(sf_last.split(':')[1:])
    return last_stat

def set_statcode(stat_rec):
    statlist = ['_','_','_','_']
    if stat_rec['A']:
        statlist[0] = 'A'
    if stat_rec['W']:
        statlist[1] = 'W'
    if stat_rec['P']:
        statlist[2] = 'P'
    if stat_rec['S']:
        statlist[3] = 'S'
    statcode = ''.join(statlist)
    stat_rec['statcode'] = statcode
    return statcode


debug = False

def main():

    assess_args()

    # split the Archive_Map into a list of records, each record a list of fields
    #   Campaign,Model,Experiment,Ensemble,DatasetType,ArchivePath,DatatypeTarExtractionPattern,Notes
    contents = loadFileLines(arch_map)
    am_list = [ aline.split(',') for aline in contents if aline[:-1] ]

    # create a sorted list of unique dataset types
    dstype_list = [ arch_rec[5] for arch_rec in am_list ]
    dstype_list = list(set(dstype_list))
    dstype_list.sort()

    # create a dictionary keyed by 'arch_loc_key' = tuple (Model,Experiment,Ensemble) with value the SET of archive path(s)
    # (Not really needed here, but nice to have, and we can reuse its keys for the dataset_status table)
    # AM_lines (old) = Camp,Model,Exper,Ensem,DSTYPE,Arch_Path,Arch_Patt
    # AM_lines (new) = Camp,Model,Exper,Resol,Ensem,DSTYPE,Arch_Path,Arch_Patt
    arch_loc_dict = {}
    for _ in am_list:
        arch_loc_dict[ tuple([_[1],_[2],_[3],_[4]]) ] = set()
    for _ in am_list:
        arch_loc_dict[ tuple([_[1],_[2],_[3],_[4]]) ] |= {_[6]}  # add another archive path to the set - may be more than one.

    # create a dictionary keyed by 'arch_loc_key', each value a dictionary keyed by dstype, value a Boolean dictionary (A,W,P)
    # For each (model,experiment,ensemble), create a dataset_type entry for every allowable dataset-type
    # initialize "Archive,Warehouse,Published" to ALL FALSE.
    dataset_status = {}
    for akey in arch_loc_dict:
        dataset_status[akey] = { dstype: { 'A': False, 'W': False, 'P': False, 'S': False } for dstype in dstype_list }
    
    # for each record in the ArchiveMap, set the corresponding [archive_key][dataset_key]['A'] = True
    for _ in am_list:
        akey = tuple([_[1],_[2],_[3],_[4]])
        dkey = _[5]
        stat_rec = dataset_status[akey][dkey]
        stat_rec['w_epath'] = ''
        stat_rec['w_maxv'] = ''
        stat_rec['w_maxc'] = 0
        stat_rec['p_epath'] = ''
        stat_rec['p_maxv'] = ''
        stat_rec['p_maxc'] = 0
        stat_rec['s_vers'] = ''
        stat_rec['scount'] = 0
        stat_rec['A'] = True
        dataset_status[akey][dkey] = stat_rec
    
    # DEBUG #####
    '''
    for akey in dataset_status:
        for dkey in dstype_list:
            print(f'(akey={akey}), (dkey={dkey}), {dataset_status[akey][dkey]}')
    sys.exit(0)
    '''
    
    ##########
    # walk the warehouse.  Need to assign "model,experiment,ensemble" to each extracted dataset (how to identify?)
    ##########

    warehouse_leaf_dirs = []
    for root, dirs, files in os.walk(warehouse):      # aggregate full sourcefile paths in src_selected
        # if not dirs and (src_selector in root):     # at leaf-directory matching src_selector
        if not dirs:     # at leaf-directory matching src_selector
            warehouse_leaf_dirs.append(root)

    wh_nonempty = []
    for adir in warehouse_leaf_dirs:
        # print(adir)
        ensdir, vleaf = os.path.split(adir)
        if not isVLeaf(vleaf):
            continue
        for root, dirs, files in os.walk(adir):
            if files:
                wh_nonempty.append( tuple([ensdir,vleaf,len(files)]))

    for atup in wh_nonempty:
        dsid = get_idval(atup[0]) # ensdir
        dsid = dsid.replace('/','.')
        # print(f'DEBUG_W: dsid = {dsid}')
        akey = get_dsid_arch_key( dsid )
        dkey = get_dsid_type_key( dsid )

        if not akey in dataset_status:  # first time we've encountered this experiment/case
            dataset_status[akey] = { dstype: { 'A': False, 'W': False, 'P': False, 'S': False } for dstype in dstype_list }
            if dkey in dataset_status[akey]:
                dataset_status[akey][dkey]['W'] = True
            else:
                dataset_status[akey][dkey] = { 'A': False, 'W': True, 'P': False, 'S': False }
        elif not dkey in dataset_status[akey]:  # first time we've encountered this dataset type for this experiment/case
            dataset_status[akey][dkey] = { 'A': False, 'W': True, 'P': False, 'S': False }
        else:  # just set Warehouse to True
            dataset_status[akey][dkey]['W'] = True

        maxv, maxc = get_maxv_info(atup[0])
        dataset_status[akey][dkey]['w_epath'] = atup[0]
        dataset_status[akey][dkey]['w_maxv'] = maxv
        dataset_status[akey][dkey]['w_maxc'] = maxc


    # DEBUG #####
    '''
    for akey in dataset_status:
        for dkey in dstype_list:
            stat_rec = dataset_status[akey][dkey]
            if stat_rec['A'] or stat_rec['W']:
                print(f'(akey={akey}), (dkey={dkey}), {dataset_status[akey][dkey]}')
            else:
                print(f'(akey={akey}), (dkey={dkey}), NO SUCH DATASET')
    sys.exit(0)
    '''
    
    ##########
    # walk the publicati.  Need to assign "model,experiment,ensemble" to each extracted dataset (how to identify?)
    ##########

    publicati_leaf_dirs = []
    for root, dirs, files in os.walk(publicati):      # aggregate full sourcefile paths in src_selected
        # if not dirs and (src_selector in root):     # at leaf-directory matching src_selector
        if not dirs:     # at leaf-directory matching src_selector
            publicati_leaf_dirs.append(root)

    pub_nonempty = []
    for adir in publicati_leaf_dirs:
        ensdir, vleaf = os.path.split(adir)
        if not isVLeaf(vleaf):
            continue
        for root, dirs, files in os.walk(adir):
            if files:
                pub_nonempty.append( tuple([ensdir,vleaf,len(files)]))

    for atup in pub_nonempty:
        dsid = get_idval(atup[0]) # ensdir
        dsid = 'E3SM.' + dsid.replace(os.sep,'.')
        # print(f'DEBUG_P: dsid = {dsid}')
        akey = get_dsid_arch_key( dsid )
        dkey = get_dsid_type_key( dsid )

        if not akey in dataset_status:  # first time we've encountered this experiment/case
            dataset_status[akey] = { dstype: { 'A': False, 'W': False, 'P': False, 'S': False } for dstype in dstype_list }
            if dkey in dataset_status[akey]:
                dataset_status[akey][dkey]['P'] = True
            else:
                dataset_status[akey][dkey] = { 'A': False, 'W': False, 'P': True, 'S': False }
        elif not dkey in dataset_status[akey]:  # first time we've encountered this dataset type for this experiment/case
            dataset_status[akey][dkey] = { 'A': False, 'W': False, 'P': True, 'S': False }
        else:  # just set Publication to True
            dataset_status[akey][dkey]['P'] = True

        if not 'w_epath' in dataset_status[akey][dkey] or not len(dataset_status[akey][dkey]['w_epath']):
            dataset_status[akey][dkey]['w_epath'] = ''
            dataset_status[akey][dkey]['w_maxv'] = ''
            dataset_status[akey][dkey]['w_maxc'] = 0

        maxv, maxc = get_maxv_info(atup[0])
        dataset_status[akey][dkey]['p_epath'] = atup[0]
        dataset_status[akey][dkey]['p_maxv'] = maxv
        dataset_status[akey][dkey]['p_maxc'] = maxc


    ##########
    # extract the datasetID from each published dataset listed in the ESGF_publication_report
    # obtain the key 'Model,Experiment,Ensemble'
    ##########

    contents = loadFileLines(esgf_pr)

    esgf_pr_list = [ aline.split(',') for aline in contents if aline[:-1] and not aline[0] == 'NOMATCH' ]

    # add S_version and S_counts
    S_count = dict()
    S_vers = dict()
    for aline in esgf_pr_list:
        tline = aline[2]
        dsid = '.'.join(tline.split('.')[:-1])
        akey = get_dsid_arch_key( dsid )
        S_count[akey] = dict()
        S_vers[akey] = dict()

    for aline in esgf_pr_list:
        filecount = aline[1]
        tline = aline[2]
        dsid = '.'.join(tline.split('.')[:-1])
        vers = tline.split('.')[-1]
        akey = get_dsid_arch_key( dsid )
        dkey = get_dsid_type_key( dsid )
        # print(f'DEBUG FC={filecount}: {akey},{dkey}')
        S_count[akey][dkey] = filecount
        S_vers[akey][dkey] = vers
    # end add S_version and S_counts

    esgf_pr_list = [ _[2] for _ in esgf_pr_list ]
    dsid_list = [ '.'.join(_.split('.')[:-1]) for _ in esgf_pr_list ]

    '''
    # DEBUG/INFO:  Report when any "published" dataset is not an archived dataset
    for id in dsid_list:
        print(f'DEBUG_S.1: id = {id}')
        akey = get_dsid_arch_key( id )
        if akey in dataset_status:
            print(f' key { akey } in dataset_status')
        else:
            print(f' key { akey } is NEW')
    '''

    # Update 'P' status of this (possibly new) dataset_type in the (possibly new) datasetID_key in the dataset_status table
    # using the dsid_list (sproket-based publication report)

    for dsid in dsid_list:
        # print(f'DEBUG_S: dsid = {dsid}')
        akey = get_dsid_arch_key( dsid )
        dkey = get_dsid_type_key( dsid )
        # print(f'DEBUG_S: akey = {akey}, dkey = {dkey}')
    
        if not akey in dataset_status:  # first time we've encountered this experiment/case
            dataset_status[akey] = { dstype: { 'A': False, 'W': False, 'P': False, 'S': False } for dstype in dstype_list }
            if dkey in dataset_status[akey]:
                dataset_status[akey][dkey]['S'] = True
            else:
                dataset_status[akey][dkey] = { 'A': False, 'W': False, 'P': False, 'S': True }
        elif not dkey in dataset_status[akey]:  # first time we've encountered this dataset type for this experiment/case
            dataset_status[akey][dkey] = { 'A': False, 'W': False, 'P': False, 'S': True }
        else:  # just set Publication to True
            dataset_status[akey][dkey]['S'] = True

        dataset_status[akey][dkey]['scount'] = S_count[akey][dkey]
        dataset_status[akey][dkey]['s_vers'] = S_vers[akey][dkey]
    
        if not 'w_epath' in dataset_status[akey][dkey] or not len(dataset_status[akey][dkey]['w_epath']):
            dataset_status[akey][dkey]['w_epath'] = ''
            dataset_status[akey][dkey]['w_maxv'] = ''
            dataset_status[akey][dkey]['w_maxc'] = 0
        if not 'p_epath' in dataset_status[akey][dkey] or not len(dataset_status[akey][dkey]['p_epath']):
            dataset_status[akey][dkey]['p_epath'] = ''
            dataset_status[akey][dkey]['p_maxv'] = ''
            dataset_status[akey][dkey]['p_maxc'] = 0

    # print all combos (model,experiment,ensemble,dataset_type)
    '''
    o   AWPS: AWPS,[opt S-count],model,experiment,ensemble,ds_type,[opt wh_path]
    n   AWPS: AWPS,model,experiment,ensemble,ds_type,w_maxv,w_maxc,p_maxv,p_maxc,s_count,w_epath,p_epath]
    '''

    repsep = ':'
    if gv_csv:
        repsep = ','

    
    for akey in dataset_status:
        for dkey in dataset_status[akey]:
            set_statcode(dataset_status[akey][dkey])

    sf_status = 'NONE'
    for akey in dataset_status:
        for dkey in dataset_status[akey]:
            stat_rec = dataset_status[akey][dkey]
            if not 'w_epath' in stat_rec:
                stat_rec['w_epath'] = ''
                stat_rec['w_maxv'] = ''
                stat_rec['w_maxc'] = 0
            if not 'p_epath' in stat_rec:
                stat_rec['p_epath'] = ''
                stat_rec['p_maxv'] = ''
                stat_rec['p_maxc'] = 0
            if not 's_vers' in stat_rec:
                stat_rec['s_vers'] = ''
                stat_rec['scount'] = 0
            # print(f'{stat_rec["statcode"]}:{akey},{dkey}:',end='')
            epath = ''
            if 'w_epath' in stat_rec and len(stat_rec['w_epath']):
                epath = stat_rec['w_epath']
                # print(f'LAST_GASP: epath (W) = {epath}')
            elif 'p_epath' in stat_rec and len(stat_rec['p_epath']):
                epath = stat_rec['p_epath']
                # print(f'LAST_GASP: epath (P) = {epath}')
            else:
                epath = ''
                # print(f'LAST_GASP: NO_PATH for {akey},{dkey}')
            sf_status = get_sf_laststat(epath)
            # print(f'DEBUG: GASP: sf_status={sf_status}: {akey},{dkey}')
            stat_rec['sf_laststat'] = sf_status
            dataset_status[akey][dkey] = stat_rec


    for akey in dataset_status:
        for dkey in dataset_status[akey]:
            stat_rec = dataset_status[akey][dkey]
            out_list = []
            out_list.append(stat_rec['statcode'])
            out_list.append(akey[0])
            out_list.append(akey[1])
            out_list.append(akey[2])
            out_list.append(akey[3])
            out_list.append(dkey)
            out_list.append(stat_rec['w_maxv'])
            out_list.append(str(stat_rec['w_maxc']))
            out_list.append(stat_rec['p_maxv'])
            out_list.append(str(stat_rec['p_maxc']))
            out_list.append(stat_rec['s_vers'])
            out_list.append(str(stat_rec['scount']))
            out_list.append(stat_rec['sf_laststat'])
            out_list.append(stat_rec['w_epath'])
            out_list.append(stat_rec['p_epath'])
            

            if gv_csv:
                out_line = ','.join(out_list)
                print(f'{out_line}')
                
    sys.exit(0)

if __name__ == "__main__":
  sys.exit(main())

